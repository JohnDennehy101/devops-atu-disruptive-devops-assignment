name: Provision GPU with Hugging Face Model Configured
on:
  pull_request:
    types: [opened, synchronize]

jobs:
  provision:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v5

      - name: Install doctl (Digital Ocean CLI)
        uses: digitalocean/action-doctl@v2
        with:
          token: ${{ secrets.DO_TOKEN }}

      - name: Get GitHub Runner Config & Digital Ocean Firewall config
        run: |
          FIREWALL_ID=$(doctl compute firewall list --format ID,Name --no-header | grep "LocalAccess" | awk '{print $1}')
          echo "FW_ID=$FIREWALL_ID" >> $GITHUB_ENV
          
          echo "RUNNER_IP=$(curl -s https://icanhazip.com)" >> $GITHUB_ENV
      
      - name: Obtain GPU ID for available GPU in target region
        id: find_h100_size
        run: |
          echo "=== All GPU sizes in target region ==="
          doctl compute size list --format Slug,Description,Regions --no-header | grep -i gpu || echo "No GPU sizes found"
          
          echo ""
          echo "=== H100 specific sizes ==="
          doctl compute size list --format Slug,Description,Regions --no-header | grep -i h100 || echo "No H100 sizes found"
          
          echo ""
          echo "=== Finding H100 with 20 vCPU and 240 GB RAM ==="
          # Try to find the exact match
          H100_SIZE=$(doctl compute size list --format Slug,Description --no-header | grep -iE "h100.*20.*240|240.*20.*h100|h100.*240.*20" | head -1 | awk '{print $1}')
          
          if [ -z "$H100_SIZE" ]; then
            # Try alternative patterns
            H100_SIZE=$(doctl compute size list --format Slug,Description --no-header | grep -i h100 | grep -iE "20|240" | head -1 | awk '{print $1}')
          fi
          
          if [ -z "$H100_SIZE" ]; then
            # Just get first H100 if available
            H100_SIZE=$(doctl compute size list --format Slug,Description --no-header | grep -i h100 | head -1 | awk '{print $1}')
          fi
          
          if [ -z "$H100_SIZE" ]; then
            echo "ERROR: Could not find H100 size. Please check the output above."
            exit 1
          fi
          
          echo "Found H100 size: $H100_SIZE"
          echo "H100_SIZE=$H100_SIZE" >> $GITHUB_ENV

      - name: Spin up GPU Droplet on Digital Ocean
        id: gpu
        run: |
          MODEL_NAME_VALUE="${{ secrets.MODEL_NAME }}"
          if [ -z "$MODEL_NAME_VALUE" ]; then
            MODEL_NAME_VALUE="Qwen/Qwen2.5-Coder-7B-Instruct"
          fi

          cat > /tmp/user-data.yml <<EOF
          write_files:
            - path: /app/llm.env
              content: |
                API_KEY=${{ secrets.LLM_API_KEY }}
                MODEL_NAME=${MODEL_NAME_VALUE}
              owner: root:root
              permissions: '0600'
          
          runcmd:
            - export API_KEY='${{ secrets.LLM_API_KEY }}'
            - export MODEL_NAME='${MODEL_NAME_VALUE}'
            - |
              $(cat ./scripts/gpu-setup.sh | tail -n +2)
          EOF
          
          DROPLET_ID=$(doctl compute droplet create ai-worker-${{ github.sha }} \
            --region ams3 \
            --size g-20vcpu-240gb-h100 \
            --image nvidia-cuda-ubuntu-22-04 \
            --user-data-file /tmp/user-data.yml \
            --wait --format ID --no-header)
          
          IP=$(doctl compute droplet get $DROPLET_ID --format PublicIPv4 --no-header)
          echo "DROPLET_ID=$DROPLET_ID" >> $GITHUB_ENV
          echo "DROPLET_IP=$IP" >> $GITHUB_ENV

      - name: Update firewall to allow current runner IP access
        run: |
          doctl compute firewall add-rules ${{ env.FW_ID }} \
            --inbound-rules "protocol:tcp,ports:8000,address:${{ env.RUNNER_IP }}/32"